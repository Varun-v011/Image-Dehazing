{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.exposure import cumulative_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SimpleITK import ReadImage, Cast, sitkInt8, sitkFloat64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SimpleITK import CannyEdgeDetection, GetArrayFromImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imshow, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.fftpack as fftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage, misc, signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import random_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.io import read_file, decode_jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.image import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haze_path = \"./Dataset/Haze/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haze_file_names_with_path_image_list = glob(haze_path+\"*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haze_file_names_with_path_image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dehaze_path = \"./Dataset/De-hazed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dehaze_file_names_with_path_image_list = glob(dehaze_path+\"*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram_matching(arr, xp, fp, clr_max=256):\n",
    "    interp_out = np.interp(arr, xp, np.arange(clr_max))   \n",
    "    pixel_result = {i:interp_out[i] for i in range(clr_max)} \n",
    "    mp = np.arange(0,clr_max)\n",
    "    for (k, v) in pixel_result.items():\n",
    "        mp[k] = v\n",
    "    s = fp.shape\n",
    "    fp = np.reshape(mp[fp.ravel()], fp.shape)\n",
    "    imp = np.reshape(fp, s)\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_distribution(im):\n",
    "    c, b = cumulative_distribution(im)\n",
    "    for i in range(b[0]):\n",
    "        c = np.insert(c, 0, 0)\n",
    "    for i in range(b[-1]+1, 256):\n",
    "        c = np.append(c, 1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_value = np.random.randint(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = imread(haze_file_names_with_path_image_list[rnd_value]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = imread(haze_file_names_with_path_image_list[rnd_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(img1), np.max(img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imga = np.zeros(img1.shape).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    cd1 = get_cumulative_distribution(img1[...,i])\n",
    "    cd2 = get_cumulative_distribution(img2[...,i])\n",
    "    imga[...,i] = get_histogram_matching(cd1, cd2, img1[...,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,17))\n",
    "plt.subplots_adjust(left=0, top=0.95, right=1, bottom=0, wspace=0.05, hspace=0.05)\n",
    "plt.subplot(221), plt.imshow(img1), plt.axis('off'), plt.title('Input Image', size=25)\n",
    "plt.subplot(222), plt.imshow(img2), plt.axis('off'), plt.title('Template Image', size=25)\n",
    "plt.subplot(223), plt.imshow(imga[...,:3]), plt.axis('off'), plt.title('Output Image', size=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ReadImage(haze_file_names_with_path_image_list[rnd_value],sitkInt8) \n",
    "img = Cast(img, sitkFloat64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_th = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_th = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges1 = CannyEdgeDetection(img, lowerThreshold=lower_th, upperThreshold=upper_th, variance=[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges2 = CannyEdgeDetection(img, lowerThreshold=lower_th, upperThreshold=upper_th, variance=[3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges3 = CannyEdgeDetection(img, lowerThreshold=lower_th, upperThreshold=upper_th, variance=[6, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = GetArrayFromImage(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges1 = GetArrayFromImage(edges1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges2 = GetArrayFromImage(edges2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges3 = GetArrayFromImage(edges3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "plt.subplot(221), plt.imshow(image.astype(np.uint8), cmap=plt.cm.gray), plt.axis('off'), plt.title('Input image', fontsize=20)\n",
    "plt.subplot(222), plt.imshow(edges1, cmap=plt.cm.gray), plt.axis('off'), plt.title('Canny filter, $\\sigma=1$', fontsize=20)\n",
    "plt.subplot(223), plt.imshow(edges2, cmap=plt.cm.gray), plt.axis('off'), plt.title('Canny filter, $\\sigma=3$', fontsize=20)\n",
    "plt.subplot(224), plt.imshow(edges3, cmap=plt.cm.gray), plt.axis('off'), plt.title('Canny filter, $\\sigma=6$', fontsize=20)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_a = np.mean(imread(haze_file_names_with_path_image_list[rnd_value]), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_kernel = np.outer(signal.gauss_spline(img_a.shape[0], 5), signal.gauss_spline(img_a.shape[1], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = fftp.fft2(img_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_kernel = fftp.fft2(fftp.ifftshift(gauss_kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved = freq*freq_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = fftp.ifft2(convolved).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_img = (20*np.log10( 0.1 + fftp.fftshift(freq))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_spectrum = (20*np.log10( 0.1 + fftp.fftshift(freq_kernel))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_kernel = (20*np.log10( 0.1 + fftp.fftshift(convolved))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.gray()\n",
    "plt.subplot(2,3,1), plt.imshow(img_a), plt.title('Original Image', size=20), plt.axis('off')\n",
    "plt.subplot(2,3,2), plt.imshow(gauss_kernel), plt.title('Gaussian Kernel', size=20)\n",
    "plt.subplot(2,3,3), plt.imshow(im1) \n",
    "plt.title('Output Image', size=20), plt.axis('off')\n",
    "plt.subplot(2,3,4), plt.imshow( output_img)\n",
    "plt.title('Original Image Spectrum', size=20), plt.axis('off')\n",
    "plt.subplot(2,3,5), plt.imshow(original_spectrum )\n",
    "plt.title('Gaussian Kernel Spectrum', size=20), plt.subplot(2,3,6)\n",
    "plt.imshow( gaussian_kernel)\n",
    "plt.title('Output Image Spectrum', size=20), plt.axis('off')\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_a = rgb2gray(imread(haze_file_names_with_path_image_list[rnd_value]))\n",
    "gauss_kernel = np.outer(signal.gauss_spline(img_a.shape[0], 1), signal.gauss_spline(img_a.shape[1], 1))\n",
    "freq = fftp.fft2(img_a)\n",
    "freq_kernel = fftp.fft2(fftp.ifftshift(gauss_kernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( (20*np.log10( 0.01 + fftp.fftshift(freq_kernel))).real.astype(int), cmap='coolwarm') \n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path):\n",
    "    img = read_file(img_path) \n",
    "    img = decode_jpeg(img, channels = 3)\n",
    "    img = resize(img, size = (384, 384), antialias = True)\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_validation_images():\n",
    "    orig_img_path = \"Dataset/De-hazed\"\n",
    "    hazy_img_path = \"Dataset/Haze\"\n",
    "    train_img = []\n",
    "    val_img = []\n",
    "\n",
    "    orig_img = glob(orig_img_path + '/*.jpg')\n",
    "    n = len(orig_img)\n",
    "    np.random.shuffle(orig_img)\n",
    "    train_keys = orig_img[:int(0.9*n)]        \n",
    "    val_keys = orig_img[int(0.9*n):]\n",
    "    \n",
    "    split_dict = {}\n",
    "    for key in train_keys:\n",
    "        split_dict[key] = 'train'\n",
    "    for key in val_keys:\n",
    "        split_dict[key] = 'val'\n",
    "\n",
    "    print(split_dict)\n",
    "    hazy_img = glob(hazy_img_path + '/*.jpg')\n",
    "    for img in hazy_img:\n",
    "        img_name = img.split('\\\\')[-1]\n",
    "        print(img_name)\n",
    "        orig_path = orig_img_path + '\\\\' + img_name.split('_')[0] + '.jpg'\n",
    "        print(orig_path)\n",
    "        if (split_dict[orig_path] == 'train'):\n",
    "            train_img.append([img, orig_path])  \n",
    "        else:\n",
    "            val_img.append([img, orig_path])\n",
    "            \n",
    "    return train_img, val_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = get_train_validation_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_data, val_data, batch_size):\n",
    "    \n",
    "    train_data_orig = tf.data.Dataset.from_tensor_slices([img[1] for img in train_data]).map(lambda x: load_image(x))\n",
    "    train_data_haze = tf.data.Dataset.from_tensor_slices([img[0] for img in train_data]).map(lambda x: load_image(x))\n",
    "    train = tf.data.Dataset.zip((train_data_haze, train_data_orig)).shuffle(buffer_size=100, reshuffle_each_iteration=True).batch(batch_size)\n",
    "    \n",
    "    val_data_orig = tf.data.Dataset.from_tensor_slices([img[1] for img in val_data]).map(lambda x: load_image(x))\n",
    "    val_data_haze = tf.data.Dataset.from_tensor_slices([img[0] for img in val_data]).map(lambda x: load_image(x))\n",
    "    val = tf.data.Dataset.zip((val_data_haze, val_data_orig)).shuffle(buffer_size=100, reshuffle_each_iteration=True).batch(batch_size)\n",
    "    \n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = load_data(train_data, val_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intializer = random_normal(stddev=0.008, seed = 101) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_initializer = tf.constant_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_regularizer = L2(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    inputs = tf.keras.Input(shape = [384, 384, 3])     \n",
    "        \n",
    "    conv = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                  bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(inputs)\n",
    "    conv = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                  bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv)\n",
    "    \n",
    "    conv_up = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv)\n",
    "    conv_up = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv_up)\n",
    "\n",
    "    conv1_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                   bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv_up)\n",
    "    conv1_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv1_1)\n",
    "    conv1_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
    "                   bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv1_2)\n",
    "    conc1 = tf.add(conv1_3, conv1_1)\n",
    "    conv1 = tf.keras.activations.relu(conc1)\n",
    "\n",
    "    conv2_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv1)\n",
    "    conv2_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv2_1)\n",
    "    conv2_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv2_2)\n",
    "    conc2 = tf.add(conv2_3, conv2_1)\n",
    "    conv2 = tf.keras.activations.relu(conc2)\n",
    "\n",
    "    conv3_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv2)\n",
    "    conv3_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv3_1)\n",
    "    conv3_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv3_2)\n",
    "    conv3_4 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv3_3)\n",
    "    conv3_5 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv3_4)\n",
    "    conc3 = tf.add(conv3_5, conv3_1)\n",
    "    conv3 = tf.keras.activations.relu(conc3)\n",
    "\n",
    "    conv4_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv3)\n",
    "    conv4_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv4_1)\n",
    "    conv4_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv4_2)\n",
    "    conv4_4 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv4_3)\n",
    "    conv4_5 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
    "                     bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv4_4)\n",
    "    conc4 = tf.add(conv4_5, conv4_1)\n",
    "    conv4 = tf.keras.activations.relu(conc4)\n",
    "\n",
    "    deconv = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
    "                             kernel_regularizer = l2_regularizer)(conv4)\n",
    "    deconv = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
    "                             kernel_regularizer = l2_regularizer)(deconv)\n",
    "\n",
    "    conv = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                  bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(deconv)\n",
    "    conv = Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
    "                  bias_initializer = bias_initializer, kernel_regularizer = l2_regularizer)(conv)\n",
    "    conc = tf.add(conv, inputs)\n",
    "    gman_output = tf.keras.activations.relu(conc)\n",
    "    \n",
    "    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 4, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                 kernel_regularizer = l2_regularizer)(inputs)\n",
    "    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 2, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                 kernel_regularizer = l2_regularizer)(conv)\n",
    "    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 2, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                 kernel_regularizer = l2_regularizer)(conv)\n",
    "    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                 kernel_regularizer = l2_regularizer)(conv)\n",
    "    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                 kernel_regularizer = l2_regularizer)(conv)\n",
    "    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 1, padding = 'same', kernel_initializer = intializer, activation = 'relu',\n",
    "                 kernel_regularizer = l2_regularizer)(conv)\n",
    "    deconv = Conv2DTranspose(filters = 64, kernel_size = 3, dilation_rate = 4, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
    "                           activation = 'relu', kernel_regularizer = l2_regularizer)(conv)\n",
    "    conv = Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n",
    "                 kernel_regularizer = l2_regularizer)(deconv)\n",
    "    conc = tf.add(conv, inputs)\n",
    "    pn_output = tf.keras.activations.relu(conc)\n",
    "    \n",
    "    output = tf.add(gman_output, pn_output)\n",
    "    \n",
    "    return Model(inputs = inputs, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = MeanSquaredError(name = \"train loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = MeanSquaredError(name = \"val loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dehaze_model(epochs, train, val, net, train_loss, val_loss, optimizer):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print(\"\\nStart of epoch %d\" % (epoch,), end=' ')\n",
    "        start_time_epoch = time.time()\n",
    "        start_time_step = time.time()\n",
    "        \n",
    "        for step, (train_batch_haze, train_batch_orig) in enumerate(train):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                train_logits = net(train_batch_haze, training = True)\n",
    "                loss = mean_squared_error(train_batch_orig, train_logits)\n",
    "\n",
    "            grads = tape.gradient(loss, net.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, net.trainable_weights))\n",
    "\n",
    "            train_loss.update_state(train_batch_orig, train_logits)\n",
    "            if step == 0:\n",
    "                print('[', end='')\n",
    "            if step % 64 == 0:\n",
    "                print('=', end='')\n",
    "        \n",
    "        print(']', end='')\n",
    "        print('  -  ', end='')\n",
    "        print('Training Loss: %.4f' % (train_loss.result()), end='')\n",
    "        \n",
    "        \n",
    "        for step, (val_batch_haze, val_batch_orig) in enumerate(val):\n",
    "            val_logits = net(val_batch_haze, training = False)\n",
    "            val_loss.update_state(val_batch_orig, val_logits)\n",
    "            \n",
    "            if step % 32 ==0:\n",
    "                display_img(net, val_batch_haze, val_batch_orig)\n",
    "        \n",
    "        print('  -  ', end='')\n",
    "        print('Validation Loss: %.4f' % (val_loss.result()), end='')\n",
    "        print('  -  ', end=' ')\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time_epoch))\n",
    "        \n",
    "        net.save('dehazed_model')           \n",
    "        train_loss.reset_states()\n",
    "        val_loss.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_dehaze_model(epochs, train, val, model, train_loss, val_loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_prediction(net, test_img_path):\n",
    "    \n",
    "    test_img = glob(test_img_path + '/*.jpg')\n",
    "    random.shuffle(test_img)\n",
    "    \n",
    "    for img in test_img:\n",
    "        \n",
    "        img = tf.io.read_file(img)\n",
    "        img = tf.io.decode_jpeg(img, channels = 3)\n",
    "        \n",
    "        if img.shape[1] > img.shape[0]:\n",
    "            img = tf.image.resize(img, size = (1080, 1920), antialias = True)\n",
    "        if img.shape[1] < img.shape[0]:\n",
    "            img = tf.image.resize(img, size = (1920, 1080), antialias = True)\n",
    "        \n",
    "        img = img / 255\n",
    "        img = tf.expand_dims(img, axis = 0)      \n",
    "        \n",
    "        dehaze = net(img, training = False)\n",
    "        \n",
    "        plt.figure(figsize = (80, 80))\n",
    "        \n",
    "        display_list = [img[0], dehaze[0]]       \n",
    "        title = ['Hazy Image', 'Dehazed Image']\n",
    "\n",
    "        for i in range(2):\n",
    "            plt.subplot(1, 2, i+1)\n",
    "            plt.title(title[i], fontsize = 65, y = 1.045)\n",
    "            plt.imshow(display_list[i])\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prediction(model, 'Dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
